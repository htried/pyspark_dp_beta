{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240951e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install poetry package manager\n",
    "\n",
    "!conda install -c conda-forge poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pydp to the local conda environment\n",
    "\n",
    "!cd ../PyDP && poetry install && cd ../pyspark_dp_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba6064",
   "metadata": {},
   "source": [
    "# General Spark Notes\n",
    "\n",
    "Two things to consider:\n",
    "- memory\n",
    "- number of cores\n",
    "\n",
    "Sizeable cluster — wmfdata gives templates of clusters you can define\n",
    "what happens on spark submit is:\n",
    "1. project gets bundled and shipped to hadoop\n",
    "2. on top of hadoop a spark cluster is created\n",
    "3. data gets split up and shipped to workers\n",
    "3. workers do computation and send results back to leader\n",
    "\n",
    "Divide total slice of input data by number of machines you want to have\n",
    "Don't want data to spill to disc — will cause a performance hit, but won't hurt the cluster\n",
    "\n",
    "wmfdata templates:\n",
    "- `yarn-large`: up to 128 workers with 8 gb of RAM and 4 threads (e.g. 4 JVMs/tasks of work) each (2gb of RAM / core) — maximum number of workers is 128 * 4 = 512 threads , each one can handle up to 2 gb of data (actually less because there's some overhead). Theoretically if there was no overhead, the `yarn-large` cluster could handle up to 1 tb of data. Spark optimizer also does data partitioning to process underlying data in chunks so it doesn't need to hold all of the data at the same time.\n",
    "- `yarn-regular`: half the size of `yarn-large`\n",
    "\n",
    "dynamic allocation: Guaranteed an upper bound of resources, but if there are higher priority jobs workers can be reallocated, which causes weird behavior. Usually at the beginning of the month there are lots of high priority jobs.\n",
    "\n",
    "Can get larger spark clusters, but need to ask engineering first — \"we're doing an experiment, can we use more?\"\n",
    "\n",
    "Can then define a larger cluster in wmfdata\n",
    "\n",
    "People to ask: Andrew Otto, Luca Toscano, Joseph Allemandou\n",
    "\n",
    "Another element is how job is written, etc:\n",
    "\n",
    "When spark ships job to executor, the execution planner tries to conduct computations on the same machine that contains the data. \"Shuffling\" is when a computation on worker C relies on data from workers A & B, and data has to be transmitted (slowly) across the network. Generally not a good practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3722bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wmfdata\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c449487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting packages...\n",
      "Packing environment at '/home/htriedman/.conda/envs/2021-06-30T22.51.28_htriedman' to 'conda-2021-06-30T22.51.28_htriedman.tgz'\n",
      "[########################################] | 100% Completed |  1min 15.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will ship conda-2021-06-30T22.51.28_htriedman.tgz to remote Spark executors.\n",
      "PySpark executors will use conda-2021-06-30T22.51.28_htriedman/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "spark = wmfdata.spark.get_session(\n",
    "    app_name='pyspark-large — differential privacy pydp — htriedman',\n",
    "    type='yarn-large',\n",
    "    ship_python_env=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abea3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get (page title, page id, project, country, actor signature) for Aug 15 2021 UTC6:00\n",
    "\n",
    "# TODO: change this to work with spark SQL, rather than RDD\n",
    "# TODO: python functions --> UDFs\n",
    "\n",
    "rdd = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  pageview_info['page_title'] as page_title,\n",
    "  page_id,\n",
    "  pageview_info['project'] as project,\n",
    "  geocoded_data['country'] as country,\n",
    "  actor_signature\n",
    "FROM wmf.pageview_actor\n",
    "WHERE\n",
    "    year = 2021 AND month = 8 AND day = 15 -- AND hour = 6\n",
    "    AND page_id IS NOT NULL\n",
    "\"\"\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdcf8bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add laplace noise to a single number\n",
    "def add_laplace_noise(x, eps, sensitivity):\n",
    "    import pydp\n",
    "    return x + pydp.distributions.LaplaceDistribution(eps, sensitivity).sample()\n",
    "\n",
    "# add laplace noise to a spark rdd\n",
    "def add_laplace_noise_to_rdd(rdd, eps, max_partitions, max_per_partition):\n",
    "    eps_per_partition = eps / max_partitions\n",
    "    sensitivity_per_partition = max_per_partition\n",
    "    return rdd.map(lambda x: (x[0], add_laplace_noise(x[1], eps_per_partition, sensitivity_per_partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db8f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gaussian noise to a single number\n",
    "def add_gaussian_noise(x, eps, delta, sensitivity):\n",
    "    import pydp\n",
    "    sigma_squared = (2 * math.log(1.25 / delta) * sensitivity**2) / (eps**2)\n",
    "    return x + pydp.distributions.GaussianDistribution(sigma_squared).sample()\n",
    "\n",
    "# add laplace noise to a spark rdd\n",
    "def add_gaussian_noise_to_rdd(rdd, eps, delta, max_partitions, max_per_partition):\n",
    "    eps_per_partition = eps / max_partitions\n",
    "    sensitivity_per_partition = max_per_partition\n",
    "    return rdd.map(lambda x: (x[0], add_gaussian_noise(x[1], eps_per_partition, delta, sensitivity_per_partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7d6f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold(eps, delta, max_partitions, max_per_partition):\n",
    "    eps_per_partition = eps / max_partitions\n",
    "    sensitivity_per_partition = max_per_partition\n",
    "    b = sensitivity_per_partition / eps_per_partition\n",
    "    return -b * math.log(2 * b * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49de218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reduceByKey is generally an expensive operation, lots of data points spread around the cluster, need to combine them all\n",
    "# TODO: optimization would largely come from using spark SQL rather than RDDs\n",
    "\n",
    "# do bounded DP count\n",
    "def do_count(rdd, eps, delta, max_partitions, max_per_partition, noise_kind):\n",
    "    # rekey to a tuple of (actor signature, page id)\n",
    "    # ((actor_signature, page_id), pageview)\n",
    "    dp_count_rdd = rdd.map(lambda x: ((x.actor_signature, x.page_id), [x]))\n",
    "\n",
    "    # randomly get a set of at most `max_per_partition` pageviews for each (actor signature, page id) tuple\n",
    "    # ((actor_signature, page_id), [pageview]) {max length of max_per_partition}\n",
    "    dp_count_rdd = dp_count_rdd.reduceByKey(lambda x, y: random.sample(x + y, min(len(x) + len(y), max_per_partition)))\n",
    "\n",
    "    # rekey to just actor signature\n",
    "    # (actor_signature, [pageview]) {with redundancies}\n",
    "    dp_count_rdd = dp_count_rdd.map(lambda x: ((x[0][0], x[1])))\n",
    "\n",
    "    # randomly get a set of at most `max_partitions` sets of pageviews for each actor signature\n",
    "    # (actor_signature, [pageview]) {max length of max_per_partition * max_partitions}\n",
    "    dp_count_rdd = dp_count_rdd.reduceByKey(lambda x, y: random.sample(x + y, min(len(x) + len(y), max_partitions)))\n",
    "\n",
    "    # drop actor signature as key\n",
    "    # ([pageview])\n",
    "    dp_count_rdd = dp_count_rdd.map(lambda x: x[1])\n",
    "\n",
    "    # unnest lists of pageviews using a flatmap\n",
    "    # (pageview)\n",
    "    dp_count_rdd = dp_count_rdd.flatMap(lambda x: x)\n",
    "\n",
    "    # now that contributions are bounded, count views per tuple\n",
    "    dp_count_rdd = dp_count_rdd.map(lambda x: ((x.project, x.country, x.page_id, x.page_title), 1))\n",
    "    dp_count_rdd = dp_count_rdd.reduceByKey(lambda x, y: (x + y))\n",
    "\n",
    "    if noise_kind == \"laplace\":\n",
    "        # add laplace noise to counts\n",
    "        dp_count_rdd = add_laplace_noise_to_rdd(dp_count_rdd, eps, max_partitions, max_per_partition)\n",
    "    elif noise_kind == \"gaussian\":\n",
    "        dp_count_rdd = add_gaussian_noise_to_rdd(dp_count_rdd, eps, delta, max_partitions, max_per_partition)\n",
    "\n",
    "    # filter tuples that have less than `min_number_of_views` views\n",
    "    dp_count_rdd = dp_count_rdd.filter(lambda x: x[1] >= calculate_threshold(delta, eps, max_partitions, max_per_partition))\n",
    "\n",
    "    # round view count to integers for readability\n",
    "    dp_count_rdd = dp_count_rdd.map(lambda x: (x[0], round(x[1], 0)))\n",
    "\n",
    "    return dp_count_rdd.takeOrdered(200, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8799e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total contributions (aka sensitivity) = max_per_partition * max_partitions\n",
    "max_partitions = 5    # say that users can visit at most 5 pages\n",
    "max_per_partition = 2 # and for each page they can contribute at most 2 pageviews\n",
    "\n",
    "eps = 1\n",
    "delta = 1 / (7e8 * math.sqrt(7e8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c979ee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing first rekey\n",
      "doing first reduceByKey\n",
      "doing second rekey\n",
      "doing second reduceByKey\n",
      "doing third rekey\n",
      "flat map\n",
      "doing thrid reduceByKey (actual count)\n",
      "adding noise\n",
      "filtering to threshold\n",
      "rounding\n",
      "taking top 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('en.wikipedia', 'United States', 15580374, 'Main_Page'), 1568856.0),\n",
       " (('de.wikipedia', 'Germany', 5248757, 'Wikipedia:Hauptseite'), 540537.0),\n",
       " (('en.wikipedia', 'United Kingdom', 15580374, 'Main_Page'), 455495.0),\n",
       " (('ja.wikipedia', 'Japan', 253348, 'メインページ'), 334953.0),\n",
       " (('en.wikipedia', 'India', 15580374, 'Main_Page'), 272602.0),\n",
       " (('fr.wikipedia', 'France', 10635368, 'Wikipédia:Accueil_principal'),\n",
       "  252129.0),\n",
       " (('de.wikipedia', 'Germany', 72746, 'Gerd_Müller'), 221146.0),\n",
       " (('en.wikipedia', 'India', 3349824, 'Vikram_Batra'), 216939.0),\n",
       " (('en.wikipedia', 'India', 30635, 'Taliban'), 214530.0),\n",
       " (('en.wikipedia', 'United States', 30635, 'Taliban'), 207496.0),\n",
       " (('de.wikipedia', 'Germany', 5070, 'Taliban'), 174477.0),\n",
       " (('en.wikipedia', 'Canada', 15580374, 'Main_Page'), 150612.0),\n",
       " (('fa.wikipedia', 'Iran', 27859, 'طالبان'), 148399.0),\n",
       " (('ru.wikipedia', 'Russia', 71896, 'Талибан'), 143396.0),\n",
       " (('ja.wikipedia', 'Japan', 3093109, 'ジャッキー・ウー'), 140920.0),\n",
       " (('it.wikipedia', 'Italy', 521472, 'Pagina_principale'), 126931.0),\n",
       " (('en.wikipedia', 'India', 2499568, 'Independence_Day_(India)'), 121557.0),\n",
       " (('en.wikipedia', 'Australia', 15580374, 'Main_Page'), 117210.0),\n",
       " (('en.wikipedia', 'United Kingdom', 30635, 'Taliban'), 113750.0),\n",
       " (('ru.wikipedia', 'Russia', 4401, 'Заглавная_страница'), 112036.0),\n",
       " (('ja.wikipedia', 'Japan', 5010, 'ターリバーン'), 111693.0),\n",
       " (('ja.wikipedia', 'Japan', 108458, '徳川昭武'), 108199.0),\n",
       " (('en.wikipedia', 'Iran', 15580374, 'Main_Page'), 104783.0),\n",
       " (('zh.wikipedia', 'United States', 3597889, 'Wikipedia:首页'), 99457.0),\n",
       " (('ja.wikipedia', 'Japan', 4061568, '石崎史郎'), 99021.0),\n",
       " (('de.wikipedia', 'Germany', 4761237, 'Ken_Miles'), 98622.0),\n",
       " (('zh.wikipedia', 'Taiwan', 6836331, '斯卡羅_(電視劇)'), 94151.0),\n",
       " (('es.wikipedia', 'United States', 2271189, 'Wikipedia:Portada'), 93023.0),\n",
       " (('tr.wikipedia', 'Turkey', 114867, 'Taliban'), 88775.0),\n",
       " (('de.wikipedia', 'Germany', 154, 'Afghanistan'), 86440.0),\n",
       " (('it.wikipedia', 'Italy', 445746, 'Ferragosto'), 86137.0),\n",
       " (('es.wikipedia', 'Spain', 2271189, 'Wikipedia:Portada'), 85893.0),\n",
       " (('it.wikipedia', 'Italy', 665216, \"Gianfranco_D'Angelo\"), 84102.0),\n",
       " (('ja.wikipedia', 'Japan', 2593279, 'DaiGo'), 80721.0),\n",
       " (('zh.wikipedia', 'Taiwan', 7301738, '俗女養成記2'), 79474.0),\n",
       " (('en.wikipedia', 'United States', 737, 'Afghanistan'), 78862.0),\n",
       " (('en.wikipedia', 'Germany', 15580374, 'Main_Page'), 78501.0),\n",
       " (('en.wikipedia', 'South Africa', 15580374, 'Main_Page'), 78280.0),\n",
       " (('ru.wikipedia', 'United States', 4401, 'Заглавная_страница'), 78257.0),\n",
       " (('fr.wikipedia', 'United States', 10635368, 'Wikipédia:Accueil_principal'),\n",
       "  78085.0),\n",
       " (('en.wikipedia', 'United States', 49632909, 'The_Suicide_Squad_(film)'),\n",
       "  77988.0),\n",
       " (('es.wikipedia', 'Mexico', 2271189, 'Wikipedia:Portada'), 77943.0),\n",
       " (('sv.wikipedia', 'Sweden', 902605, 'Portal:Huvudsida'), 76475.0),\n",
       " (('en.wikipedia', 'India', 737, 'Afghanistan'), 75117.0),\n",
       " (('ja.wikipedia', 'United States', 253348, 'メインページ'), 75026.0),\n",
       " (('pt.wikipedia', 'United States', 952, 'Wikipédia:Página_principal'),\n",
       "  74078.0),\n",
       " (('de.wikipedia', 'United States', 5248757, 'Wikipedia:Hauptseite'), 73663.0),\n",
       " (('nl.wikipedia', 'Netherlands', 722, 'Hoofdpagina'), 71953.0),\n",
       " (('it.wikipedia', 'United States', 521472, 'Pagina_principale'), 71045.0),\n",
       " (('en.wikipedia', 'India', 1425939, 'Indian_Idol'), 66465.0),\n",
       " (('it.wikipedia', 'Italy', 65507, 'Talebani'), 65279.0),\n",
       " (('en.wikipedia', 'United States', 20646122, 'Index.html'), 64504.0),\n",
       " (('en.wikipedia', 'India', 60600284, 'Shershaah'), 62213.0),\n",
       " (('en.wikipedia', 'India', 60318741, 'Bhuj:_The_Pride_of_India'), 60426.0),\n",
       " (('zh.wikipedia', 'Taiwan', 4314, '塔利班'), 58489.0),\n",
       " (('commons.wikimedia', 'United States', 1, 'Main_Page'), 57594.0),\n",
       " (('fr.wikipedia', 'France', 4727507, 'Taliban'), 56630.0),\n",
       " (('en.wikipedia',\n",
       "   'Brazil',\n",
       "   46230181,\n",
       "   'Saudi_Arabian-led_intervention_in_Yemen'),\n",
       "  56542.0),\n",
       " (('en.wikipedia', 'Brazil', 9087364, 'Islamic_State_of_Iraq_and_the_Levant'),\n",
       "  56460.0),\n",
       " (('en.wikipedia',\n",
       "   'Brazil',\n",
       "   57590513,\n",
       "   \"2015–16_ISU_World_Standings_and_Season's_World_Ranking\"),\n",
       "  56386.0),\n",
       " (('en.wikipedia',\n",
       "   'Brazil',\n",
       "   38892181,\n",
       "   'List_of_United_States_counties_and_county_equivalents'),\n",
       "  56329.0),\n",
       " (('en.wikipedia',\n",
       "   'Brazil',\n",
       "   54266373,\n",
       "   'Special_Counsel_investigation_(2017–2019)'),\n",
       "  56254.0),\n",
       " (('en.wikipedia', 'Brazil', 48969854, '2017_in_American_television'),\n",
       "  56237.0),\n",
       " (('pl.wikipedia', 'Poland', 68805, 'Talibowie'), 56194.0),\n",
       " (('en.wikipedia',\n",
       "   'Brazil',\n",
       "   55175661,\n",
       "   'List_of_2018–19_Super_Rugby_transfers'),\n",
       "  56172.0),\n",
       " (('en.wikipedia', 'Brazil', 48980006, '2016_in_aviation'), 56170.0),\n",
       " (('en.wikipedia',\n",
       "   'Brazil',\n",
       "   4644339,\n",
       "   'List_of_United_States_Representatives_from_New_York'),\n",
       "  56144.0),\n",
       " (('en.wikipedia', 'Brazil', 52925867, '2018_in_American_television'),\n",
       "  56134.0),\n",
       " (('en.wikipedia', 'Brazil', 44486544, '2015_in_British_television'), 56132.0),\n",
       " (('en.wikipedia', 'Brazil', 4848272, 'Donald_Trump'), 56117.0),\n",
       " (('en.wikipedia',\n",
       "   'Brazil',\n",
       "   54115568,\n",
       "   'Timeline_of_Russian_interference_in_the_2016_United_States_elections'),\n",
       "  56064.0),\n",
       " (('en.wikipedia', 'Brazil', 236034, 'List_of_municipalities_in_Michigan'),\n",
       "  56035.0),\n",
       " (('de.wikipedia', 'Switzerland', 5248757, 'Wikipedia:Hauptseite'), 55975.0),\n",
       " (('en.wikipedia', 'Brazil', 55547578, '2018_in_arthropod_paleontology'),\n",
       "  55939.0),\n",
       " (('en.wikipedia', 'Brazil', 50034356, 'Panama_Papers'), 55830.0),\n",
       " (('en.wikipedia',\n",
       "   'Brazil',\n",
       "   15910,\n",
       "   'List_of_compositions_by_Johann_Sebastian_Bach'),\n",
       "  55805.0),\n",
       " (('en.wikipedia',\n",
       "   'Brazil',\n",
       "   56207055,\n",
       "   'List_of_2010s_deaths_in_rock_and_roll'),\n",
       "  55711.0),\n",
       " (('ja.wikipedia', 'Japan', 4512, 'アフガニスタン'), 55544.0),\n",
       " (('it.wikipedia', 'Italy', 287867, 'Gerd_Müller'), 54363.0),\n",
       " (('fr.wikipedia', 'France', 37581, 'Assomption_de_Marie'), 53702.0),\n",
       " (('en.wikipedia', 'United States', 60203476, 'Free_Guy'), 52948.0),\n",
       " (('ja.wikipedia', 'Japan', 22741, '終戦の日'), 52876.0),\n",
       " (('pl.wikipedia', 'Poland', 2956678, 'Wikipedia:Strona_główna'), 51948.0),\n",
       " (('de.wikipedia', 'Germany', 10383248, 'Le_Mans_66_–_Gegen_jede_Chance'),\n",
       "  50760.0),\n",
       " (('en.wikipedia', 'India', 60827, 'Cleopatra'), 50229.0),\n",
       " (('nl.wikipedia', 'Netherlands', 42246, 'Taliban'), 48112.0),\n",
       " (('ru.wikipedia', 'Ukraine', 71896, 'Талибан'), 47947.0),\n",
       " (('en.wikipedia', 'United Kingdom', 172313, 'Gerd_Müller'), 46192.0),\n",
       " (('de.wikipedia', 'Austria', 5248757, 'Wikipedia:Hauptseite'), 46095.0),\n",
       " (('it.wikipedia', 'Italy', 6125958, 'Tammy_Abraham'), 45862.0),\n",
       " (('en.wikipedia', 'United States', 68395966, 'Tyler_Gilbert'), 45690.0),\n",
       " (('it.wikipedia', 'Italy', 1625285, 'Afghanistan'), 45234.0),\n",
       " (('en.wikipedia', 'India', 978203, 'Flag_of_India'), 44712.0),\n",
       " (('fr.wikipedia', 'France', 89360, 'Gerd_Müller'), 42735.0),\n",
       " (('pt.wikipedia', 'Brazil', 952, 'Wikipédia:Página_principal'), 42676.0),\n",
       " (('en.wikipedia', 'United States', 65625519, 'The_White_Lotus'), 42551.0),\n",
       " (('ja.wikipedia', 'Japan', 1145395, 'ジェリー藤尾'), 42234.0),\n",
       " (('en.wikipedia', 'United States', 404323, 'Ashraf_Ghani'), 42159.0),\n",
       " (('ja.wikipedia', 'Japan', 456928, 'ノースアジア大学明桜高等学校'), 41788.0),\n",
       " (('ru.wikipedia', 'Russia', 1116, 'Афганистан'), 41565.0),\n",
       " (('en.wikipedia', 'United States', 60577671, 'Beckett_(film)'), 41479.0),\n",
       " (('en.wikipedia', 'United States', 48347765, \"Don't_Breathe\"), 41188.0),\n",
       " (('ja.wikipedia', 'Japan', 2069252, '鍛治舎巧'), 40976.0),\n",
       " (('ja.wikipedia', 'Japan', 2826002, '唐橋ユミ'), 40606.0),\n",
       " (('sv.wikipedia', 'Sweden', 42673, 'Taliban'), 40407.0),\n",
       " (('en.wikipedia', 'Netherlands', 15580374, 'Main_Page'), 39462.0),\n",
       " (('en.wikipedia',\n",
       "   'United States',\n",
       "   19401090,\n",
       "   '2008_South_Carolina_Learjet_60_crash'),\n",
       "  39383.0),\n",
       " (('en.wikipedia', 'India', 56280, 'Jana_Gana_Mana'), 38832.0),\n",
       " (('ja.wikipedia', 'Japan', 3840659, '板垣李光人'), 38780.0),\n",
       " (('de.wikipedia', 'Germany', 10180837, 'Pornhub'), 38499.0),\n",
       " (('en.wikipedia', 'Sweden', 15580374, 'Main_Page'), 38066.0),\n",
       " (('en.wikipedia', 'Philippines', 15580374, 'Main_Page'), 37982.0),\n",
       " (('en.wikipedia', 'United Arab Emirates', 15580374, 'Main_Page'), 37947.0),\n",
       " (('en.wikipedia', 'United Kingdom', 737, 'Afghanistan'), 37786.0),\n",
       " (('tr.wikipedia', 'Turkey', 2740662, 'Anasayfa'), 37459.0),\n",
       " (('fr.wikipedia', 'France', 17958, 'Jean-Pierre_Bacri'), 37252.0),\n",
       " (('ja.wikipedia', 'Japan', 1069644, '鈴木亮平_(俳優)'), 37222.0),\n",
       " (('en.wikipedia', 'India', 803754, 'Kargil_War'), 36984.0),\n",
       " (('en.wikipedia', 'Pakistan', 15580374, 'Main_Page'), 36295.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   54115568,\n",
       "   'Timeline_of_Russian_interference_in_the_2016_United_States_elections'),\n",
       "  36259.0),\n",
       " (('ru.wikipedia', 'Russia', 73715, 'Таривердиев,_Микаэл_Леонович'), 36134.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   236034,\n",
       "   'List_of_municipalities_in_Michigan'),\n",
       "  36108.0),\n",
       " (('en.wikipedia', 'Philippines', 48969854, '2017_in_American_television'),\n",
       "  36103.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   54266373,\n",
       "   'Special_Counsel_investigation_(2017–2019)'),\n",
       "  36101.0),\n",
       " (('ja.wikipedia', 'Japan', 3571127, '東京卍リベンジャーズ'), 36067.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   55175661,\n",
       "   'List_of_2018–19_Super_Rugby_transfers'),\n",
       "  36059.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   56207055,\n",
       "   'List_of_2010s_deaths_in_rock_and_roll'),\n",
       "  36035.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   57590513,\n",
       "   \"2015–16_ISU_World_Standings_and_Season's_World_Ranking\"),\n",
       "  35991.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   9087364,\n",
       "   'Islamic_State_of_Iraq_and_the_Levant'),\n",
       "  35959.0),\n",
       " (('en.wikipedia', 'Philippines', 52925867, '2018_in_American_television'),\n",
       "  35947.0),\n",
       " (('en.wikipedia', 'Philippines', 55547578, '2018_in_arthropod_paleontology'),\n",
       "  35922.0),\n",
       " (('en.wikipedia', 'Philippines', 4848272, 'Donald_Trump'), 35910.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   46230181,\n",
       "   'Saudi_Arabian-led_intervention_in_Yemen'),\n",
       "  35882.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   15910,\n",
       "   'List_of_compositions_by_Johann_Sebastian_Bach'),\n",
       "  35823.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   4644339,\n",
       "   'List_of_United_States_Representatives_from_New_York'),\n",
       "  35819.0),\n",
       " (('it.wikipedia', 'Italy', 1603963, 'Assunzione_di_Maria'), 35793.0),\n",
       " (('en.wikipedia', 'Philippines', 48980006, '2016_in_aviation'), 35792.0),\n",
       " (('en.wikipedia',\n",
       "   'Philippines',\n",
       "   38892181,\n",
       "   'List_of_United_States_counties_and_county_equivalents'),\n",
       "  35784.0),\n",
       " (('en.wikipedia', 'Philippines', 44486544, '2015_in_British_television'),\n",
       "  35727.0),\n",
       " (('en.wikipedia', 'Philippines', 50034356, 'Panama_Papers'), 35694.0),\n",
       " (('en.wikipedia', 'United States', 53327, 'Val_Kilmer'), 35501.0),\n",
       " (('ja.wikipedia', 'Japan', 188411, '高川学園高等学校・中学校'), 35381.0),\n",
       " (('en.wikipedia', 'United States', 147367, 'Aretha_Franklin'), 35135.0),\n",
       " (('fr.wikipedia', 'Russia', 478594, '4L_Trophy'), 34806.0),\n",
       " (('en.wikivoyage', 'Russia', 1729, 'Ashtabula'), 34507.0),\n",
       " (('zh.wikipedia', 'Hong Kong', 4314, '塔利班'), 34013.0),\n",
       " (('en.wikipedia', 'India', 404323, 'Ashraf_Ghani'), 33964.0),\n",
       " (('ru.wikiquote', 'Russia', 2462, 'Марк_Туллий_Цицерон'), 33804.0),\n",
       " (('en.wikivoyage', 'Russia', 10449, 'East_Greenwich'), 33778.0),\n",
       " (('de.wikipedia', 'Germany', 174409, 'Blues_Brothers'), 33756.0),\n",
       " (('fr.wikipedia', 'Russia', 1034876, 'Questionnaire'), 33538.0),\n",
       " (('en.wikipedia', 'United States', 1135721, 'Fall_of_Saigon'), 33461.0),\n",
       " (('fr.wikipedia', 'Russia', 1641742, 'Saint_Ignace'), 33388.0),\n",
       " (('en.wikipedia', 'France', 15580374, 'Main_Page'), 33351.0),\n",
       " (('fr.wikipedia', 'France', 23, 'Afghanistan'), 33145.0),\n",
       " (('de.wikipedia', 'Germany', 522613, 'Carroll_Shelby'), 33032.0),\n",
       " (('ja.wikipedia', 'Japan', 1254634, 'イモトアヤコ'), 33015.0),\n",
       " (('tr.wikipedia', 'Turkey', 3204, 'Afganistan'), 32926.0),\n",
       " (('fr.wikipedia', 'France', 1206328, 'Les_Blues_Brothers_(film)'), 32924.0),\n",
       " (('en.wikipedia', 'United States', 408285, 'Nanci_Griffith'), 32558.0),\n",
       " (('it.wikipedia', 'Italy', 1546928, 'Camila_Giorgi'), 32533.0),\n",
       " (('en.wikipedia', 'United States', 216774, 'Layne_Staley'), 32215.0),\n",
       " (('zh.wikipedia', 'Taiwan', 3597889, 'Wikipedia:首页'), 32164.0),\n",
       " (('en.wikipedia', 'Singapore', 15580374, 'Main_Page'), 31998.0),\n",
       " (('en.wikipedia', 'Australia', 30635, 'Taliban'), 31796.0),\n",
       " (('en.wikipedia', 'India', 1384520, 'XXXX'), 31528.0),\n",
       " (('ml.wikipedia', 'India', 42375, 'ഒന്നാം_ഇന്ത്യൻ_സ്വാതന്ത്ര്യ_സമരം_(1857)'),\n",
       "  31491.0),\n",
       " (('pt.wikipedia', 'Brazil', 192967, 'Glória_Menezes'), 31445.0),\n",
       " (('zh.wikipedia', 'Taiwan', 8300, '阿富汗'), 31392.0),\n",
       " (('ja.wikipedia', 'Japan', 108451, '渋沢栄一'), 31168.0),\n",
       " (('vi.wikipedia', 'Vietnam', 116732, 'Taliban'), 31089.0),\n",
       " (('da.wikipedia', 'Denmark', 77, 'Forside'), 30855.0),\n",
       " (('en.wikipedia', 'Canada', 30635, 'Taliban'), 30477.0),\n",
       " (('en.wikipedia', 'United States', 41109403, 'Attacks_on_parachutists'),\n",
       "  30468.0),\n",
       " (('en.wikipedia', 'United States', 34017673, 'The_Fear_Index'), 30319.0),\n",
       " (('pl.wikipedia', 'Poland', 44827, 'Afganistan'), 30184.0),\n",
       " (('en.wikipedia', 'India', 9087364, 'Islamic_State_of_Iraq_and_the_Levant'),\n",
       "  30106.0),\n",
       " (('ar.wikipedia', 'Saudi Arabia', 1413, 'طالبان'), 30086.0),\n",
       " (('it.wikipedia', 'Italy', 129532, 'Piera_Degli_Esposti'), 29709.0),\n",
       " (('zh.wikipedia', 'Taiwan', 3825859, '斯卡羅'), 29268.0),\n",
       " (('en.wikipedia', 'United Kingdom', 57467048, 'Trevoh_Chalobah'), 29206.0),\n",
       " (('en.wikipedia', 'Malaysia', 15580374, 'Main_Page'), 29145.0),\n",
       " (('ja.wikipedia', 'Japan', 728695, '神戸国際大学附属高等学校'), 28773.0),\n",
       " (('en.wikipedia', 'India', 4848272, 'Donald_Trump'), 28619.0),\n",
       " (('en.wikipedia', 'India', 50034356, 'Panama_Papers'), 28608.0),\n",
       " (('en.wikipedia', 'India', 236034, 'List_of_municipalities_in_Michigan'),\n",
       "  28505.0),\n",
       " (('fa.wikipedia', 'Iran', 2, 'صفحهٔ_اصلی'), 28411.0),\n",
       " (('en.wikipedia', 'India', 52925867, '2018_in_American_television'), 28356.0),\n",
       " (('en.wikipedia',\n",
       "   'India',\n",
       "   46230181,\n",
       "   'Saudi_Arabian-led_intervention_in_Yemen'),\n",
       "  28339.0),\n",
       " (('en.wikipedia', 'India', 55547578, '2018_in_arthropod_paleontology'),\n",
       "  28251.0),\n",
       " (('en.wikipedia', 'India', 48969854, '2017_in_American_television'), 28224.0),\n",
       " (('en.wikipedia', 'Bangladesh', 15580374, 'Main_Page'), 28209.0),\n",
       " (('en.wikipedia', 'India', 48980006, '2016_in_aviation'), 28196.0),\n",
       " (('fr.wikipedia', 'France', 8674, 'Haïti'), 28179.0),\n",
       " (('en.wikipedia',\n",
       "   'India',\n",
       "   4644339,\n",
       "   'List_of_United_States_Representatives_from_New_York'),\n",
       "  28094.0),\n",
       " (('en.wikipedia',\n",
       "   'India',\n",
       "   54115568,\n",
       "   'Timeline_of_Russian_interference_in_the_2016_United_States_elections'),\n",
       "  28086.0),\n",
       " (('en.wikipedia',\n",
       "   'India',\n",
       "   38892181,\n",
       "   'List_of_United_States_counties_and_county_equivalents'),\n",
       "  28066.0),\n",
       " (('en.wikipedia',\n",
       "   'India',\n",
       "   15910,\n",
       "   'List_of_compositions_by_Johann_Sebastian_Bach'),\n",
       "  28025.0),\n",
       " (('en.wikipedia', 'India', 55175661, 'List_of_2018–19_Super_Rugby_transfers'),\n",
       "  28013.0),\n",
       " (('hi.wikipedia', 'India', 10409, 'तालिबान_आन्दोलन'), 27942.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_count(rdd, eps, delta, max_partitions, max_per_partition, \"laplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b28193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275.5415406595753\n"
     ]
    }
   ],
   "source": [
    "print(calculate_threshold(eps, delta, max_partitions, max_per_partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cdba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_count(rdd, eps, delta, max_partitions, max_per_partition, \"gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32664fd0",
   "metadata": {},
   "source": [
    "# Spark SQL implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67d30269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91eab4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  pageview_info['page_title'] as page_title,\n",
    "  page_id,\n",
    "  pageview_info['project'] as project,\n",
    "  geocoded_data['country'] as country,\n",
    "  actor_signature\n",
    "FROM wmf.pageview_actor\n",
    "WHERE\n",
    "    year = 2021 AND month = 8 AND day = 15 -- AND hour = 6\n",
    "    AND page_id IS NOT NULL\n",
    "\"\"\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97626d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add laplace noise to a single number\n",
    "@udf(returnType=FloatType())\n",
    "def add_laplace_noise(x, eps, sensitivity):\n",
    "    import pydp\n",
    "    return x + pydp.distributions.LaplaceDistribution(eps, sensitivity).sample()\n",
    "\n",
    "# add laplace noise to a spark df\n",
    "def add_laplace_noise_to_df(df, eps, max_partitions, max_per_partition):\n",
    "    eps_per_partition = eps / max_partitions\n",
    "    sensitivity_per_partition = max_per_partition\n",
    "    \n",
    "    return df.withColumn(\"Laplacian_Noisy_Count\", add_laplace_noise(\"count\", eps_per_partition, sensitivity_per_partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8f34143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gaussian noise to a single number\n",
    "@udf(returnType=FloatType())\n",
    "def add_gaussian_noise(x, eps, delta, sensitivity):\n",
    "    import pydp\n",
    "    sigma_squared = (2 * math.log(1.25 / delta) * sensitivity**2) / (eps**2)\n",
    "    return x + pydp.distributions.GaussianDistribution(sigma_squared).sample()\n",
    "\n",
    "# add laplace noise to a spark df\n",
    "def add_gaussian_noise_to_df(df, eps, delta, max_partitions, max_per_partition):\n",
    "    eps_per_partition = eps / max_partitions\n",
    "    sensitivity_per_partition = max_per_partition\n",
    "    \n",
    "    return df.withColumn(\"Gaussian_Noisy_Count\", add_gaussian_noise(\"count\", eps_per_partition, sensitivity_per_partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a784a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold(eps, delta, max_partitions, max_per_partition):\n",
    "    eps_per_partition = eps / max_partitions\n",
    "    sensitivity_per_partition = max_per_partition\n",
    "    b = sensitivity_per_partition / eps_per_partition\n",
    "    return -b * math.log(2 * b * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57766f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reduceByKey is generally an expensive operation, lots of data points spread around the cluster, need to combine them all\n",
    "# TODO: optimization would largely come from using spark SQL rather than RDDs\n",
    "\n",
    "# do bounded DP count\n",
    "def do_count(rdd, eps, delta, max_partitions, max_per_partition, noise_kind):\n",
    "    # rekey to a tuple of (actor signature, page id)\n",
    "    # ((actor_signature, page_id), pageview)\n",
    "    dp_count_rdd = rdd.map(lambda x: ((x.actor_signature, x.page_id), [x]))\n",
    "\n",
    "    # randomly get a set of at most `max_per_partition` pageviews for each (actor signature, page id) tuple\n",
    "    # ((actor_signature, page_id), [pageview]) {max length of max_per_partition}\n",
    "    dp_count_rdd = dp_count_rdd.reduceByKey(lambda x, y: random.sample(x + y, min(len(x) + len(y), max_per_partition)))\n",
    "\n",
    "    # rekey to just actor signature\n",
    "    # (actor_signature, [pageview]) {with redundancies}\n",
    "    dp_count_rdd = dp_count_rdd.map(lambda x: ((x[0][0], x[1])))\n",
    "\n",
    "    # randomly get a set of at most `max_partitions` sets of pageviews for each actor signature\n",
    "    # (actor_signature, [pageview]) {max length of max_per_partition * max_partitions}\n",
    "    dp_count_rdd = dp_count_rdd.reduceByKey(lambda x, y: random.sample(x + y, min(len(x) + len(y), max_partitions)))\n",
    "\n",
    "    # drop actor signature as key\n",
    "    # ([pageview])\n",
    "    dp_count_rdd = dp_count_rdd.map(lambda x: x[1])\n",
    "\n",
    "    # unnest lists of pageviews using a flatmap\n",
    "    # (pageview)\n",
    "    dp_count_rdd = dp_count_rdd.flatMap(lambda x: x)\n",
    "    \n",
    "    df = dp_count_rdd.toDF()\n",
    "\n",
    "    # now that contributions are bounded, count views per tuple\n",
    "    df = df.groupBy(['project', 'country', 'page_id', 'page_title']).count()\n",
    "    \n",
    "    thresh = calculate_threshold(delta, eps, max_partitions, max_per_partition)\n",
    "\n",
    "    # add laplace noise to counts, filter to threshold, round, and return the df\n",
    "    if noise_kind == \"laplace\":\n",
    "        df = add_laplace_noise_to_df(df, eps, max_partitions, max_per_partition)\n",
    "        df = df.filter(df.Laplacian_Noisy_Count > thresh)\n",
    "        df = df.foreach(lambda x: round(x.Laplacian_Noisy_Count, 0))\n",
    "        return df.sort(df.Laplacian_Noisy_Count.desc()).take(500)\n",
    "    elif noise_kind == \"gaussian\":\n",
    "        df = add_gaussian_noise_to_df(df, eps, delta, max_partitions, max_per_partition)\n",
    "        df = df.filter(df.Gaussian_Noisy_Count > thresh)\n",
    "        df = df.foreach(lambda x: round(x.Gaussian_Noisy_Count, 0))\n",
    "        return df.sort(df.Gaussian_Noisy_Count.desc()).take(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "130ebf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total contributions (aka sensitivity) = max_per_partition * max_partitions\n",
    "max_partitions = 5    # say that users can visit at most 5 pages\n",
    "max_per_partition = 2 # and for each page they can contribute at most 2 pageviews\n",
    "\n",
    "eps = 1\n",
    "delta = 1 / (7e8 * math.sqrt(7e8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e70c4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275.5415406595753\n"
     ]
    }
   ],
   "source": [
    "print(calculate_threshold(eps, delta, max_partitions, max_per_partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11e907cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: 0.2 of type <class 'float'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-37eacb11b37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_partitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_per_partition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"laplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-676be39e5f32>\u001b[0m in \u001b[0;36mdo_count\u001b[0;34m(rdd, eps, delta, max_partitions, max_per_partition, noise_kind)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# add laplace noise to counts, filter to threshold, round, and return the df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnoise_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"laplace\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_laplace_noise_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_partitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_per_partition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLaplacian_Noisy_Count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLaplacian_Noisy_Count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-28d21fa4d032>\u001b[0m in \u001b[0;36madd_laplace_noise_to_df\u001b[0;34m(df, eps, max_partitions, max_per_partition)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msensitivity_per_partition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_per_partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Laplacian_Noisy_Count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_laplace_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_per_partition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensitivity_per_partition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# This function is for improving the online help system in the interactive interpreter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m\"For column literals, use 'lit', 'array', 'struct' or 'create_map' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \"function.\".format(col, type(col)))\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: 0.2 of type <class 'float'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "do_count(rdd, eps, delta, max_partitions, max_per_partition, \"laplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f57d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_count(rdd, eps, delta, max_partitions, max_per_partition, \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61b7e53e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f212cf794d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampleBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfractions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqlContext' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
    "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
    "sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17805919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
